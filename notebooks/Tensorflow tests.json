{"paragraphs":[{"text":"%pyspark\nimport sys\nprint(sc.version)\nprint(sys.version)","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"editorMode":"ace/mode/python","results":{},"editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404305_-875420943","id":"20170321-102536_1353198708","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:16941"},{"title":"Initialize ZeppelinSession ...","text":"%pyspark\n\nfrom zeppelin_session import ZeppelinSession, resetZeppelinSession, LogLevel, Logger\n\nresetZeppelinSession(z.z)\n\n# LogLevel().setLogLevel(\"DEBUG\")\nzs = ZeppelinSession(z.z)\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404306_-874266696","id":"20170320-190838_55365813","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16942"},{"title":"... and start it in the next paragraph","text":"%pyspark\n\nzs.start()","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404307_-874651445","id":"20170320-190902_1117826594","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16943"},{"text":"%pyspark\n\nclass Breaker:\n    def __init__(self, zeppelinContext):\n        self.z = zeppelinContext\n        print(\"\"\"%angular\\n<button id=\"__breaker__\" ng-click=\"__stop__ = 1\">Stop</button>\"\"\")\n    \n    def start(self):\n         self.z.angularBind(\"__stop__\", 0)\n        \n    def isStopped(self):\n        return self.z.angular(\"__stop__\") == 1\n\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"results":[],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404308_-876575190","id":"20170320-211052_28428655","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16944"},{"title":"Initialize NVD3","text":"%pyspark\n\nfrom zeppelin_viz.nvd3 import Nvd3\nfrom zeppelin_viz.nvd3.nvd3_chart import Nvd3Chart\n\nnv = Nvd3()\n\nnv.reloadNVD3(\"1.8.5\")\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404309_-876959939","id":"20170320-191608_451907126","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16945"},{"title":"Tensorflow example ...","text":"%pyspark\n\nimport tensorflow as tf\nimport math\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\ntf.set_random_seed(0)\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404309_-876959939","id":"20170320-184513_258887949","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16946"},{"title":"... example training the MNIST data","text":"%pyspark\n\nmnist = read_data_sets(\"/opt/data/mnist\", one_hot=True, reshape=False, validation_size=0)","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404310_-875805692","id":"20170320-184533_365540253","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16947"},{"title":"Model 1 (5 layer sigmoid)","text":"%pyspark\n\n# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\n\n# correct answers will go here\nY_ = tf.placeholder(tf.float32, [None, 10])\n\n# five layers and their number of neurons (tha last layer has 10 softmax neurons)\nL = 200\nM = 100\nN = 60\nO = 30\n\n# Weights initialised with small random values between -0.2 and +0.2\n# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\nW1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\nB1 = tf.Variable(tf.zeros([L]))\nW2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\nB2 = tf.Variable(tf.zeros([M]))\nW3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\nB3 = tf.Variable(tf.zeros([N]))\nW4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\nB4 = tf.Variable(tf.zeros([O]))\nW5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\nB5 = tf.Variable(tf.zeros([10]))\n\n\nXX = tf.reshape(X, [-1, 784])\nY1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\nY2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\nY3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + B3)\nY4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + B4)\nYlogits = tf.matmul(Y4, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\n\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# All weights for visualization\nallweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\nallbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\n\n# training step, learning rate = 0.003\nlearning_rate = 0.003\ntrain_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n\nmodel = 1\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[],"editorMode":"ace/mode/python","editorHide":false,"editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404311_-876190441","id":"20170320-184652_1589511078","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16948"},{"title":"Model 2 (5 layer relu, lr decay, dropout)","text":"%pyspark\n\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\n# correct answers will go here\nY_ = tf.placeholder(tf.float32, [None, 10])\n\n# variable learning rate\nlr = tf.placeholder(tf.float32)\n\n# Probability of keeping a node during dropout = 1.0 at test time (no dropout) and 0.75 at training time\npkeep = tf.placeholder(tf.float32)\n\n# five layers and their number of neurons (tha last layer has 10 softmax neurons)\nL = 200\nM = 100\nN = 60\nO = 30\n# Weights initialised with small random values between -0.2 and +0.2\n# When using RELUs, make sure biases are initialised with small *positive* values for example 0.1 = tf.ones([K])/10\nW1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\nB1 = tf.Variable(tf.ones([L])/10)\nW2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\nB2 = tf.Variable(tf.ones([M])/10)\nW3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\nB3 = tf.Variable(tf.ones([N])/10)\nW4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\nB4 = tf.Variable(tf.ones([O])/10)\nW5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\nB5 = tf.Variable(tf.zeros([10]))\n\n# The model, with dropout at each layer\nXX = tf.reshape(X, [-1, 28*28])\n\nY1 = tf.nn.relu(tf.matmul(XX, W1) + B1)\nY1d = tf.nn.dropout(Y1, pkeep)\n\nY2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\nY2d = tf.nn.dropout(Y2, pkeep)\n\nY3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\nY3d = tf.nn.dropout(Y3, pkeep)\n\nY4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\nY4d = tf.nn.dropout(Y4, pkeep)\n\nYlogits = tf.matmul(Y4d, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\n# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n# problems with log(0) which is NaN\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nallweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\nallbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\n\n# training step, the learning rate is a placeholder\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n\nmodel = 2\n\n\n\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[],"editorMode":"ace/mode/python","editorHide":false,"editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404312_-878114185","id":"20170320-184721_207897504","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16949"},{"title":"Model 3 (5 layer convolutional, relu, lr decay, dropout)","text":"%pyspark\n\n# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\n# correct answers will go here\nY_ = tf.placeholder(tf.float32, [None, 10])\n# variable learning rate\nlr = tf.placeholder(tf.float32)\n# test flag for batch norm\ntst = tf.placeholder(tf.bool)\niter = tf.placeholder(tf.int32)\n# dropout probability\npkeep = tf.placeholder(tf.float32)\npkeep_conv = tf.placeholder(tf.float32)\n\ndef batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n    bnepsilon = 1e-5\n    if convolutional:\n        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n    else:\n        mean, variance = tf.nn.moments(Ylogits, [0])\n    update_moving_everages = exp_moving_avg.apply([mean, variance])\n    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n    return Ybn, update_moving_everages\n\ndef no_batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n    return Ylogits, tf.no_op()\n\ndef compatible_convolutional_noise_shape(Y):\n    noiseshape = tf.shape(Y)\n    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])\n    return noiseshape\n\n# three convolutional layers with their channel counts, and a\n# fully connected layer (tha last layer has 10 softmax neurons)\nK = 24  # first convolutional layer output depth\nL = 48  # second convolutional layer output depth\nM = 64  # third convolutional layer\nN = 200  # fully connected layer\n\nW1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\nB1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\nW2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\nB2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\nW3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\nB3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n\nW4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\nB4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\nW5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\nB5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n\n# The model\n# batch norm scaling is not useful with relus\n# batch norm offsets are used instead of biases\nstride = 1  # output is 28x28\nY1l = tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME')\nY1bn, update_ema1 = batchnorm(Y1l, tst, iter, B1, convolutional=True)\nY1r = tf.nn.relu(Y1bn)\nY1 = tf.nn.dropout(Y1r, pkeep_conv, compatible_convolutional_noise_shape(Y1r))\nstride = 2  # output is 14x14\nY2l = tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME')\nY2bn, update_ema2 = batchnorm(Y2l, tst, iter, B2, convolutional=True)\nY2r = tf.nn.relu(Y2bn)\nY2 = tf.nn.dropout(Y2r, pkeep_conv, compatible_convolutional_noise_shape(Y2r))\nstride = 2  # output is 7x7\nY3l = tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME')\nY3bn, update_ema3 = batchnorm(Y3l, tst, iter, B3, convolutional=True)\nY3r = tf.nn.relu(Y3bn)\nY3 = tf.nn.dropout(Y3r, pkeep_conv, compatible_convolutional_noise_shape(Y3r))\n\n# reshape the output from the third convolution for the fully connected layer\nYY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n\nY4l = tf.matmul(YY, W4)\nY4bn, update_ema4 = batchnorm(Y4l, tst, iter, B4)\nY4r = tf.nn.relu(Y4bn)\nY4 = tf.nn.dropout(Y4r, pkeep)\nYlogits = tf.matmul(Y4, W5) + B5\nY = tf.nn.softmax(Ylogits)\n\nupdate_ema = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)\n\n# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images\n# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n# problems with log(0) which is NaN\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\ncross_entropy = tf.reduce_mean(cross_entropy)*100\n\n# accuracy of the trained model, between 0 (worst) and 1 (best)\ncorrect_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# matplotlib visualisation\nallweights = tf.concat([tf.reshape(W1, [-1]), tf.reshape(W2, [-1]), tf.reshape(W3, [-1]), tf.reshape(W4, [-1]), tf.reshape(W5, [-1])], 0)\nallbiases  = tf.concat([tf.reshape(B1, [-1]), tf.reshape(B2, [-1]), tf.reshape(B3, [-1]), tf.reshape(B4, [-1]), tf.reshape(B5, [-1])], 0)\nconv_activations = tf.concat([tf.reshape(tf.reduce_max(Y1r, [0]), [-1]), tf.reshape(tf.reduce_max(Y2r, [0]), [-1]), tf.reshape(tf.reduce_max(Y3r, [0]), [-1])], 0)\ndense_activations = tf.reduce_max(Y4r, [0])\n\n\n# training step, the learning rate is a placeholder\ntrain_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n\nmodel = 3\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"editorMode":"ace/mode/python","results":{},"editorHide":false,"editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404312_-878114185","id":"20170321-095544_960868384","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16950"},{"title":"The training function","text":"%pyspark\ndef train(train_period, batch_size):\n    for i in range(training_steps+1):\n\n        # training on batches of 100 images with 100 labels\n        batch_X, batch_Y = mnist.train.next_batch(100)\n\n        if model == 2:\n            # learning rate decay\n            max_learning_rate = 0.003\n            min_learning_rate = 0.0001\n            decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n        elif model == 3:\n            max_learning_rate = 0.02\n            min_learning_rate = 0.0001\n            decay_speed = 1600\n            learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n\n        # compute test values for visualisation\n        if i % batch_size == 0:\n            if model == 1:\n                a_test, l_test = sess.run([accuracy, cross_entropy], {X: mnist.test.images, Y_: mnist.test.labels})\n            elif model == 2:\n                a_test, l_test = sess.run([accuracy, cross_entropy], {X: mnist.test.images, Y_: mnist.test.labels, pkeep:1.0})\n            elif model == 3:\n                a_test, l_test = sess.run([accuracy, cross_entropy], {X: mnist.test.images, Y_: mnist.test.labels, pkeep: 1.0, pkeep_conv: 1.0, tst: True})\n\n        # compute training values for visualisation\n        if i % train_period == 0:\n            if model == 1:\n                a_train, l_train, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], {X: batch_X, Y_: batch_Y})\n            elif model == 2:\n                a_train, l_train, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], {X: batch_X, Y_: batch_Y, pkeep:1.0})\n            elif model == 3:\n                a_train, l_train, w, b = sess.run([accuracy, cross_entropy, allweights, allbiases], {X: batch_X, Y_: batch_Y, pkeep: 1.0, pkeep_conv: 1.0, tst: False})\n\n            alAppend(i, a_train, a_test, l_train, l_test)\n\n        if breaker.isStopped():\n            print(\"Interrupted\")\n            break\n        \n        # the backpropagation training step\n        if model == 1:  \n            sess.run(train_step, {X: batch_X, Y_: batch_Y})\n        elif model == 2:\n            sess.run(train_step, {X: batch_X, Y_: batch_Y, pkeep: 0.75, lr: learning_rate})\n        elif model == 3:\n            sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, tst: False, pkeep: 0.75, pkeep_conv: 1.0})\n            sess.run(update_ema, {X: batch_X, Y_: batch_Y, tst: False, iter: i, pkeep: 1.0, pkeep_conv: 1.0})\n","user":"anonymous","dateUpdated":"2017-03-29T15:22:05+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793511698_1124088442","id":"20170329-151831_1515719707","dateCreated":"2017-03-29T15:18:31+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16951"},{"title":"Step 1: Initialise Model","text":"%pyspark\n\n# init\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nif model in [1,2]:\n    training_steps = 10000\nelif model == 3:\n    training_steps = 5000\n\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404314_-877344688","id":"20170320-184935_1115960941","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16952"},{"title":"Step 2: Prepare Visualisations","text":"%pyspark\n\nalChart = nv.lineChart()\n\ndef convert(typ, x, y_test, y_train):\n    return alChart.convert(data={\"step\":[x], \"train_%s\"%typ:[y_train], \"test_%s\"%typ:[y_test]},\n                           key=\"step\", values=[\"train_%s\"%typ, \"test_%s\"%typ])\n    \n# first \"artifical\" points\nlmax = 20\naData = convert(\"accuracy\", 0, 0, 0)\nlData = convert(\"loss\",     0, lmax, lmax)\n\ntest_color = nv.c10()[0]\ntrain_color = nv.c10()[1]\n\naConfig={\"height\":500, \"width\": 500, \"color\":[train_color, test_color], \n         \"xAxis\":{\"axisLabel\":\"Step\", \"tickFormat\":\",d\"},             \"xDomain\":[0,training_steps], \n         \"yAxis\": {\"axisLabel\":\"Accuracy\", \"tickFormat\":\",.3f\"},      \"yDomain\":[0.94,1]}\n\nlConfig={\"height\":500, \"width\": 500, \"color\":[train_color, test_color],\n         \"xAxis\":{\"axisLabel\":\"Step\", \"tickFormat\":\",d\"},             \"xDomain\":[0,training_steps], \n         \"yAxis\": {\"axisLabel\":\"Cross Entropy\", \"tickFormat\":\",.3f\"}, \"yDomain\":[0,lmax]}\n\n# create plots\nalChart.plot([{\"data\":aData, \"config\":aConfig}, {\"data\":lData, \"config\":lConfig}])\n\ndef alAppend(i, a_train, l_train, a_test, l_test):\n    data = convert(\"accuracy\", i, a_test, a_train)\n    alChart.append({\"data\":data, \"config\":aConfig}, chart=0)\n\n    data = convert(\"loss\", i, min(lmax, l_test), min(lmax, l_train))\n    alChart.append({\"data\":data, \"config\":lConfig}, chart=1)\n\n\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404316_-879653181","id":"20170320-190740_184779135","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16953"},{"title":"Save charts","text":"%pyspark\nimport time\nalChart.saveAsPng(\"accuracy%d.png\" % model, chart=0)\ntime.sleep(3)\nalChart.saveAsPng(\"loss%d.png\" % model, chart=1)\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404317_-880037930","id":"20170320-215911_2018209483","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16954"},{"title":"The interrupt button","text":"%pyspark\n\nbreaker = Breaker(z.z)\n\n","dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404318_-878883683","id":"20170320-211913_877016685","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16955"},{"title":"Step 3: Start Learning","text":"%pyspark\n\nbreaker.start()\n\ntrain_period = 20\nbatch_size = 5 * train_period\n\ntrain(train_period, batch_size)\n","dateUpdated":"2017-03-29T15:22:05+0200","config":{"enabled":true,"title":true,"results":[],"editorMode":"ace/mode/python","editorSetting":{"language":"python"},"colWidth":12},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404318_-878883683","id":"20170320-184956_312642282","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16956"},{"dateUpdated":"2017-03-29T15:16:44+0200","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1490793404319_-879268432","id":"20170320-185044_1983535247","dateCreated":"2017-03-29T15:16:44+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:16957"}],"name":"Tensorflow tests","id":"2CCBUARY1","angularObjects":{"2CDNFBMGA:shared_process":[],"2CCZEM8GY:shared_process":[],"2CCEZZU57:shared_process":[],"2CEPA7GWA:shared_process":[],"2CDEZTV3Y:shared_process":[],"2CBTVUKD8:shared_process":[],"2CDQ1TN3G:shared_process":[],"2CBJ3JDPV:shared_process":[],"2CDADYZPC:shared_process":[],"2CDMAWYTG:shared_process":[],"2CCPMSEBH:shared_process":[],"2CBDN5VDU:shared_process":[],"2CBXVZ89X:shared_process":[],"2CC1G649T:shared_process":[],"2CEQTHQNU:shared_process":[],"2CE2HBWUD:shared_process":[],"2CEJFVVJM:shared_process":[],"2CET6ZSSR:shared_process":[],"2CC5X9SE4:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}